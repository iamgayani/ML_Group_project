---
title: "term project"
output:
  pdf_document: default
  html_document: default
date: "2024-11-28"
---
```{r}
options(repos = c(CRAN = "https://mirrors.dotsrc.org/cran/"))
install.packages("dplyr")
library(dplyr)
train <- read.csv("train.csv")
test <- read.csv("test.csv")

ols_model <- lm(log_pSat_Pa ~ ., data = train) 

test_target <- predict(ols_model, newdata = test)

submission <- data.frame(ID = test$ID, TARGET = test_target)

write.csv(submission, "ols_submission.csv", row.names = FALSE)
```

Score:0.7163
In this part, after building the OLS model, I wanted to make the model more accurate or improve its score. So I tried two methods. The first method was to convert the parentspecies text column in the training data into numbers, but the result stayed the same. The second method was to analyze the correlation between each column and the target, selecting columns with a correlation above 0.5 or 0.3, and then using these for the model prediction. However, even after trying these two methods, the prediction score didn’t change, so I’m not sure what might be going wrong

Gradient Boosting
```{r}
#install.packages(c("data.table", "xgboost", "dplyr"))

library(data.table)
library(xgboost)
library(dplyr)
```
```{r}
train <- fread("train.csv")
test <- fread("test.csv")

train[is.na(train)] <- 0
test[is.na(test)] <- 0

train <- train %>% select(-parentspecies)
test <- test %>% select(-parentspecies)

X_train <- train %>% select(-log_pSat_Pa, -ID)
y_train <- train$log_pSat_Pa 
X_test <- test %>% select(-ID)
```

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test))

params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8
)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100, 
  watchlist = list(train = dtrain),
  verbose = 1
)
```

```{r}
test$log_pSat_Pa <- predict(xgb_model, newdata = dtest)


submission <- data.frame(ID = test$ID, TARGET = test$log_pSat_Pa)
write.csv(submission, "xgb_submission.csv", row.names = FALSE)
```
Score:0.7472

tuning process
```{r}
install.packages(c("xgboost", "caret", "dplyr"))
library(xgboost)
library(caret)
library(dplyr)
```
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")

train[is.na(train)] <- 0
test[is.na(test)] <- 0

X_train <- train %>% select(-c(log_pSat_Pa, ID, parentspecies))
y_train <- train$log_pSat_Pa
X_test <- test %>% select(-c(ID, parentspecies))

X_train_matrix <- as.matrix(X_train)
X_test_matrix <- as.matrix(X_test)
```
```{r}
grid <- expand.grid(
  nrounds = c(100, 200),
  max_depth = c(3, 6, 9),
  eta = c(0.01, 0.1, 0.2),
  gamma = c(0, 1, 5),
  colsample_bytree = c(0.7, 0.8, 1),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.7, 0.8, 1) 
)

control <- trainControl(
  method = "cv",
  number = 5, 
  verboseIter = TRUE
)

xgb_tuned <- train(
  x = X_train_matrix, y = y_train,
  method = "xgbTree",
  trControl = control,
  tuneGrid = grid
)

print(xgb_tuned$bestTune)

test_predictions_tuned <- predict(xgb_tuned, X_test_matrix)

submission_tuned <- data.frame(ID = test$ID, TARGET = test_predictions_tuned)
write.csv(submission_tuned, "xgb_tuned_submission.csv", row.names = FALSE)
```
Score:0.7456



svm
```{r}
install.packages("e1071")
install.packages("caret")
library(e1071)
library(caret)
```
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")

train[is.na(train)] <- 0
test[is.na(test)] <- 0

X_train <- train[, !(names(train) %in% c("log_pSat_Pa", "ID", "parentspecies"))]
y_train <- train$log_pSat_Pa
X_test <- test[, !(names(test) %in% c("ID", "parentspecies"))]
```

```{r}
svm_model <- svm(
  x = X_train, y = y_train,
  type = "eps-regression",
  kernel = "radial",
  cost = 1,
  gamma = 1 / ncol(X_train)
)

test_predictions <- predict(svm_model, X_test)

submission <- data.frame(ID = test$ID, TARGET = test_predictions)
write.csv(submission, "svm_submission.csv", row.names = FALSE)

```
Score:0.7494
tuning process
```{r}
tune_grid <- expand.grid(
  sigma = c(0.001, 0.01, 0.1),
  C = c(0.1, 1, 10)
)

control <- trainControl(method = "cv", number = 5)

svm_tuned <- train(
  x = X_train, y = y_train,
  method = "svmRadial",       
  tuneGrid = tune_grid,
  trControl = control,
  preProcess = c("center", "scale")
)

print(svm_tuned$bestTune)
print(svm_tuned)

test_predictions_tuned <- predict(svm_tuned, X_test)

submission_tuned <- data.frame(ID = test$ID, TARGET = test_predictions_tuned)
write.csv(submission_tuned, "svm_tuned_submission.csv", row.names = FALSE)
```
Score:0.7532
